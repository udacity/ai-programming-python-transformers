{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "CMWMOaDg-ZBv"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Use CUDA if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "text = Path('../data/tiny-shakespeare.txt').read_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "IwwOe-tJ-xcE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text[0:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Ap_Ixr0M-0Yv"
   },
   "outputs": [],
   "source": [
    "\n",
    "class CharTokenizer:\n",
    "  def __init__(self, vocabulary):\n",
    "    self.token_id_for_char = {char: token_id for token_id, char in enumerate(vocabulary)}\n",
    "    self.char_for_token_id = {token_id: char for token_id, char in enumerate(vocabulary)}\n",
    "\n",
    "  @staticmethod\n",
    "  def train_from_text(text):\n",
    "    vocabulary = set(text)\n",
    "    return CharTokenizer(sorted(list(vocabulary)))\n",
    "\n",
    "  def encode(self, text):\n",
    "    token_ids = []\n",
    "    for char in text:\n",
    "      token_ids.append(self.token_id_for_char[char])\n",
    "    return torch.tensor(token_ids, dtype=torch.long)\n",
    "\n",
    "  def decode(self, token_ids):\n",
    "    chars = []\n",
    "    for token_id in token_ids.tolist():\n",
    "      chars.append(self.char_for_token_id[token_id])\n",
    "    return ''.join(chars)\n",
    "\n",
    "\n",
    "  def vocabulary_size(self):\n",
    "    return len(self.token_id_for_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "T3q9Dj3l-2Ja"
   },
   "outputs": [],
   "source": [
    "tokenizer = CharTokenizer.train_from_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Lb1zImZr-4mY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([20, 43, 50, 50, 53,  1, 61, 53, 56, 50, 42])\n",
      "Hello world\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode(\"Hello world\"))\n",
    "print(tokenizer.decode(tokenizer.encode(\"Hello world\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "MlTEiIqs-7Uz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 65\n"
     ]
    }
   ],
   "source": [
    "print(f\"Vocabulary size: {tokenizer.vocabulary_size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "7Qal76ig-94U"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class TokenIdsDataset(Dataset):\n",
    "  def __init__(self, data, block_size):\n",
    "    self.data = data\n",
    "    self.block_size = block_size\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data) - self.block_size\n",
    "\n",
    "  def __getitem__(self, pos):\n",
    "    assert pos < len(self.data) - self.block_size\n",
    "\n",
    "    x = self.data[pos:pos + self.block_size]\n",
    "    y = self.data[pos + 1:pos + 1 + self.block_size]\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "  \"vocabulary_size\": tokenizer.vocabulary_size(),\n",
    "  \"context_size\": 256,\n",
    "  \"embedding_dim\": 768,\n",
    "  \"heads_num\": 12,\n",
    "  \"layers_num\": 10,\n",
    "  \"dropout_rate\": 0.1,\n",
    "  \"use_bias\": False,\n",
    "}\n",
    "\n",
    "config[\"head_size\"] = config[\"embedding_dim\"] // config[\"heads_num\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "  def __init__(self, config):\n",
    "    super().__init__()\n",
    "    self.Q_weights = nn.Linear(config[\"embedding_dim\"], config[\"head_size\"], config[\"use_bias\"])\n",
    "    self.K_weights = nn.Linear(config[\"embedding_dim\"], config[\"head_size\"], config[\"use_bias\"])\n",
    "    self.V_weights = nn.Linear(config[\"embedding_dim\"], config[\"head_size\"], config[\"use_bias\"])\n",
    "\n",
    "    self.dropout = nn.Dropout(config[\"dropout_rate\"])\n",
    "\n",
    "    casual_attention_mask = torch.tril(torch.ones(config[\"context_size\"], config[\"context_size\"]))\n",
    "    self.register_buffer('casual_attention_mask', casual_attention_mask)\n",
    "\n",
    "  def forward(self, input):\n",
    "    batch_size, tokens_num, embedding_dim = input.shape\n",
    "    Q = self.Q_weights(input)\n",
    "    K = self.K_weights(input)\n",
    "    V = self.V_weights(input)\n",
    "\n",
    "    attention_scores = Q @ K.transpose(1, 2)\n",
    "    attention_scores = attention_scores.masked_fill(\n",
    "        self.casual_attention_mask[:tokens_num,:tokens_num] == 0,\n",
    "        -torch.inf\n",
    "    )\n",
    "    attention_scores = attention_scores / ( K.shape[-1] ** 0.5 )\n",
    "    attention_scores = torch.softmax(attention_scores, dim=-1)\n",
    "    attention_scores = self.dropout(attention_scores)\n",
    "\n",
    "    return attention_scores @ V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.rand(8, config[\"context_size\"], config[\"embedding_dim\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ah = AttentionHead(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = ah(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 256, 64])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "  def __init__(self, config):\n",
    "    super().__init__()\n",
    "\n",
    "    heads_list = [AttentionHead(config) for _ in range(config[\"heads_num\"])]\n",
    "    self.heads = nn.ModuleList(heads_list)\n",
    "\n",
    "    self.linear = nn.Linear(config[\"embedding_dim\"], config[\"embedding_dim\"])\n",
    "    self.dropout = nn.Dropout(config[\"dropout_rate\"])\n",
    "\n",
    "  def forward(self, input):\n",
    "    # print(f\"Input shape: {input.shape}\")\n",
    "    heads_outputs = [head(input) for head in self.heads]\n",
    "\n",
    "    scores_change = torch.cat(heads_outputs, dim=-1)\n",
    "    # print(f\"heads shape: {scores_change.shape}\")\n",
    "\n",
    "    scores_change = self.linear(scores_change)\n",
    "    return self.dropout(scores_change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "mha = MultiHeadAttention(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.rand(8, config[\"context_size\"], config[\"embedding_dim\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = mha(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 256, 768])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "\n",
    "  def __init__(self, config):\n",
    "    super().__init__()\n",
    "\n",
    "    self.linear_layers = nn.Sequential(\n",
    "        nn.Linear(config[\"embedding_dim\"], config[\"embedding_dim\"] * 4),\n",
    "        nn.GELU(),\n",
    "        nn.Linear(config[\"embedding_dim\"] * 4, config[\"embedding_dim\"]),\n",
    "        nn.Dropout(config[\"dropout_rate\"])\n",
    "    )\n",
    "\n",
    "  def forward(self, input):\n",
    "    return self.linear_layers(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff = FeedForward(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.rand(8, config[\"context_size\"], config[\"embedding_dim\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ouptut = ff(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 256, 768])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "\n",
    "  def __init__(self, config):\n",
    "    super().__init__()\n",
    "\n",
    "    self.multi_head = MultiHeadAttention(config)\n",
    "    self.layer_norm_1 = nn.LayerNorm(config[\"embedding_dim\"])\n",
    "\n",
    "    self.feed_forward = FeedForward(config)\n",
    "    self.layer_norm_2 = nn.LayerNorm(config[\"embedding_dim\"])\n",
    "\n",
    "  def forward(self, input):\n",
    "    residual = input\n",
    "    x = self.multi_head(self.layer_norm_1(input))\n",
    "    x = x + residual\n",
    "\n",
    "    residual = x\n",
    "    x = self.feed_forward(self.layer_norm_2(x))\n",
    "    return x + residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = Block(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ouptut = b(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 256, 768])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DemoGPT(nn.Module):\n",
    "  def __init__(self, config):\n",
    "    super().__init__()\n",
    "\n",
    "    self.token_embedding_layer = nn.Embedding(config[\"vocabulary_size\"], config[\"embedding_dim\"])\n",
    "    self.positional_embedding_layer = nn.Embedding(config[\"context_size\"], config[\"embedding_dim\"])\n",
    "\n",
    "    blocks = [Block(config) for _ in range(config[\"layers_num\"])]\n",
    "    self.layers = nn.Sequential(*blocks)\n",
    "\n",
    "    self.layer_norm = nn.LayerNorm(config[\"embedding_dim\"])\n",
    "    self.unembedding = nn.Linear(config[\"embedding_dim\"], config[\"vocabulary_size\"], bias=False)\n",
    "\n",
    "  def forward(self, token_ids):\n",
    "    batch_size, tokens_num = token_ids.shape\n",
    "\n",
    "    x = self.token_embedding_layer(token_ids)\n",
    "    sequence = torch.arange(tokens_num, device=device)\n",
    "    x = x + self.positional_embedding_layer(sequence)\n",
    "\n",
    "    x = self.layers(x)\n",
    "    x = self.layer_norm(x)\n",
    "    x = self.unembedding(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DemoGPT(config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(tokenizer.encode(\"Hi\").unsqueeze(dim=0).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 65])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, prompt_ids, max_tokens):\n",
    "    output_ids = prompt_ids\n",
    "    for _ in range(max_tokens):\n",
    "      if output_ids.shape[1] >= config[\"context_size\"]:\n",
    "        break\n",
    "      with torch.no_grad():\n",
    "        logits = model(output_ids)\n",
    "\n",
    "      logits = logits[:, -1, :]\n",
    "      probs = F.softmax(logits, dim=-1)\n",
    "      # Sample a random token given the softmax distribution\n",
    "      next_token_id = torch.multinomial(probs, num_samples=1)\n",
    "      # Add new token to the output, and repeat the process\n",
    "      output_ids = torch.cat([output_ids, next_token_id], dim=-1)\n",
    "    return output_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_prompt(model, tokenizer, prompt, max_tokens=100):\n",
    "  model.eval()\n",
    "\n",
    "  prompt = tokenizer.encode(prompt).unsqueeze(dim=0).to(device)\n",
    "\n",
    "  return tokenizer.decode(generate(model, prompt, max_tokens=max_tokens)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"First Citizen:\\nDc?SlGAOau$r'QE'us3:nSfYjnkJpFkLVEc$GovIKqlQHxRa\\nA;MWyCrmtZkMp?jCiqNPWFutrO $YcaR:rb!BdGtPyvE$-PVVsE\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_with_prompt(model, tokenizer, \"First Citizen:\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
