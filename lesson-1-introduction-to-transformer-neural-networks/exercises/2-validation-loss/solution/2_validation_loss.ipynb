{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CMWMOaDg-ZBv"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Use CUDA if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "text = Path('../../../data/tiny-shakespeare.txt').read_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "IwwOe-tJ-xcE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text[0:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ap_Ixr0M-0Yv"
   },
   "outputs": [],
   "source": [
    "\n",
    "class CharTokenizer:\n",
    "  def __init__(self, vocabulary):\n",
    "    self.token_id_for_char = {char: token_id for token_id, char in enumerate(vocabulary)}\n",
    "    self.char_for_token_id = {token_id: char for token_id, char in enumerate(vocabulary)}\n",
    "\n",
    "  @staticmethod\n",
    "  def train_from_text(text):\n",
    "    vocabulary = set(text)\n",
    "    return CharTokenizer(sorted(list(vocabulary)))\n",
    "\n",
    "  def encode(self, text):\n",
    "    token_ids = []\n",
    "    for char in text:\n",
    "      token_ids.append(self.token_id_for_char[char])\n",
    "    return torch.tensor(token_ids, dtype=torch.long)\n",
    "\n",
    "  def decode(self, token_ids):\n",
    "    chars = []\n",
    "    for token_id in token_ids.tolist():\n",
    "      chars.append(self.char_for_token_id[token_id])\n",
    "    return ''.join(chars)\n",
    "\n",
    "\n",
    "  def vocabulary_size(self):\n",
    "    return len(self.token_id_for_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T3q9Dj3l-2Ja"
   },
   "outputs": [],
   "source": [
    "tokenizer = CharTokenizer.train_from_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lb1zImZr-4mY"
   },
   "outputs": [],
   "source": [
    "print(tokenizer.encode(\"Hello world\"))\n",
    "print(tokenizer.decode(tokenizer.encode(\"Hello world\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MlTEiIqs-7Uz"
   },
   "outputs": [],
   "source": [
    "print(f\"Vocabulary size: {tokenizer.vocabulary_size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Qal76ig-94U"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class TokenIdsDataset(Dataset):\n",
    "  def __init__(self, data, block_size):\n",
    "    self.data = data\n",
    "    self.block_size = block_size\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data) - self.block_size\n",
    "\n",
    "  def __getitem__(self, pos):\n",
    "    assert pos < len(self.data) - self.block_size\n",
    "\n",
    "    x = self.data[pos:pos + self.block_size]\n",
    "    y = self.data[pos + 1:pos + 1 + self.block_size]\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "  \"vocabulary_size\": tokenizer.vocabulary_size(),\n",
    "  \"context_size\": 256,\n",
    "  \"d_embed\": 768,\n",
    "  \"heads_num\": 12,\n",
    "  \"layers_num\": 10,\n",
    "  \"dropout_rate\": 0.1,\n",
    "  \"use_bias\": False,\n",
    "}\n",
    "\n",
    "config[\"head_size\"] = config[\"d_embed\"] // config[\"heads_num\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "  def __init__(self, config):\n",
    "    super().__init__()\n",
    "    self.Q_weights = nn.Linear(config[\"d_embed\"], config[\"head_size\"], config[\"use_bias\"])\n",
    "    self.K_weights = nn.Linear(config[\"d_embed\"], config[\"head_size\"], config[\"use_bias\"])\n",
    "    self.V_weights = nn.Linear(config[\"d_embed\"], config[\"head_size\"], config[\"use_bias\"])\n",
    "\n",
    "    self.dropout = nn.Dropout(config[\"dropout_rate\"])\n",
    "\n",
    "    casual_attention_mask = torch.tril(torch.ones(config[\"context_size\"], config[\"context_size\"]))\n",
    "    self.register_buffer('casual_attention_mask', casual_attention_mask)\n",
    "\n",
    "  def forward(self, input):\n",
    "    batch_size, tokens_num, d_embed = input.shape\n",
    "    Q = self.Q_weights(input)\n",
    "    K = self.K_weights(input)\n",
    "    V = self.V_weights(input)\n",
    "\n",
    "    attention_scores = Q @ K.transpose(1, 2)\n",
    "    attention_scores = attention_scores.masked_fill(\n",
    "        self.casual_attention_mask[:tokens_num,:tokens_num] == 0,\n",
    "        -torch.inf\n",
    "    )\n",
    "    attention_scores = attention_scores / ( K.shape[-1] ** 0.5 )\n",
    "    attention_scores = torch.softmax(attention_scores, dim=-1)\n",
    "    attention_scores = self.dropout(attention_scores)\n",
    "\n",
    "    return attention_scores @ V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.rand(8, config[\"context_size\"], config[\"d_embed\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ah = AttentionHead(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = ah(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "  def __init__(self, config):\n",
    "    super().__init__()\n",
    "\n",
    "    heads_list = [AttentionHead(config) for _ in range(config[\"heads_num\"])]\n",
    "    self.heads = nn.ModuleList(heads_list)\n",
    "\n",
    "    self.linear = nn.Linear(config[\"d_embed\"], config[\"d_embed\"])\n",
    "    self.dropout = nn.Dropout(config[\"dropout_rate\"])\n",
    "\n",
    "  def forward(self, input):\n",
    "    # print(f\"Input shape: {input.shape}\")\n",
    "    heads_outputs = [head(input) for head in self.heads]\n",
    "\n",
    "    scores_change = torch.cat(heads_outputs, dim=-1)\n",
    "    # print(f\"heads shape: {scores_change.shape}\")\n",
    "\n",
    "    scores_change = self.linear(scores_change)\n",
    "    return self.dropout(scores_change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mha = MultiHeadAttention(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.rand(8, config[\"context_size\"], config[\"d_embed\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = mha(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "\n",
    "  def __init__(self, config):\n",
    "    super().__init__()\n",
    "\n",
    "    self.linear_layers = nn.Sequential(\n",
    "        nn.Linear(config[\"d_embed\"], config[\"d_embed\"] * 4),\n",
    "        nn.GELU(),\n",
    "        nn.Linear(config[\"d_embed\"] * 4, config[\"d_embed\"]),\n",
    "        nn.Dropout(config[\"dropout_rate\"])\n",
    "    )\n",
    "\n",
    "  def forward(self, input):\n",
    "    return self.linear_layers(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff = FeedForward(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.rand(8, config[\"context_size\"], config[\"d_embed\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ouptut = ff(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "\n",
    "  def __init__(self, config):\n",
    "    super().__init__()\n",
    "\n",
    "    self.multi_head = MultiHeadAttention(config)\n",
    "    self.layer_norm_1 = nn.LayerNorm(config[\"d_embed\"])\n",
    "\n",
    "    self.feed_forward = FeedForward(config)\n",
    "    self.layer_norm_2 = nn.LayerNorm(config[\"d_embed\"])\n",
    "\n",
    "  def forward(self, input):\n",
    "    residual = input\n",
    "    x = self.multi_head(self.layer_norm_1(input))\n",
    "    x = x + residual\n",
    "\n",
    "    residual = x\n",
    "    x = self.feed_forward(self.layer_norm_2(x))\n",
    "    return x + residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = Block(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ouptut = b(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DemoGPT(nn.Module):\n",
    "  def __init__(self, config):\n",
    "    super().__init__()\n",
    "\n",
    "    self.token_embedding_layer = nn.Embedding(config[\"vocabulary_size\"], config[\"d_embed\"])\n",
    "    self.positional_embedding_layer = nn.Embedding(config[\"context_size\"], config[\"d_embed\"])\n",
    "\n",
    "    blocks = [Block(config) for _ in range(config[\"layers_num\"])]\n",
    "    self.layers = nn.Sequential(*blocks)\n",
    "\n",
    "    self.layer_norm = nn.LayerNorm(config[\"d_embed\"])\n",
    "    self.unembedding = nn.Linear(config[\"d_embed\"], config[\"vocabulary_size\"], bias=False)\n",
    "\n",
    "  def forward(self, token_ids):\n",
    "    # print(\"Forward\")\n",
    "    batch_size, tokens_num = token_ids.shape\n",
    "\n",
    "    x = self.token_embedding_layer(token_ids)\n",
    "    sequence = torch.arange(tokens_num, device=device)\n",
    "    x = x + self.positional_embedding_layer(sequence)\n",
    "\n",
    "    x = self.layers(x)\n",
    "    x = self.layer_norm(x)\n",
    "    x = self.unembedding(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DemoGPT(config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(tokenizer.encode(\"Hi\").unsqueeze(dim=0).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, prompt_ids, max_tokens):\n",
    "    output_ids = prompt_ids\n",
    "    for _ in range(max_tokens):\n",
    "      if output_ids.shape[1] >= config[\"context_size\"]:\n",
    "        break\n",
    "      with torch.no_grad():\n",
    "        logits = model(output_ids)\n",
    "\n",
    "      logits = logits[:, -1, :]\n",
    "      probs = F.softmax(logits, dim=-1)\n",
    "      # Sample a random token given the softmax distribution\n",
    "      next_token_id = torch.multinomial(probs, num_samples=1)\n",
    "      # Add new token to the output, and repeat the process\n",
    "      output_ids = torch.cat([output_ids, next_token_id], dim=-1)\n",
    "    return output_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_prompt(model, tokenizer, prompt, max_tokens=100):\n",
    "  model.eval()\n",
    "\n",
    "  prompt = tokenizer.encode(prompt).unsqueeze(dim=0).to(device)\n",
    "\n",
    "  return tokenizer.decode(generate(model, prompt, max_tokens=max_tokens)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_with_prompt(model, tokenizer, \"First Citizen:\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_iterations = 500\n",
    "evaluation_interval = 10\n",
    "learning_rate=4e-4\n",
    "train_split = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text = tokenizer.encode(text).to(device)\n",
    "train_count = int(train_split * len(tokenized_text))\n",
    "train_data, validation_data = tokenized_text[:train_count], tokenized_text[train_count:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TokenIdsDataset(train_data, config[\"context_size\"])\n",
    "validation_dataset = TokenIdsDataset(validation_data, config[\"context_size\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, RandomSampler\n",
    "\n",
    "train_sampler = RandomSampler(train_dataset, num_samples=batch_size * train_iterations, replacement=True)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "\n",
    "validation_sampler = RandomSampler(validation_dataset, replacement=True)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=batch_size, sampler=validation_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute validation loss for the model using \"batches_num\" batches\n",
    "# from the validation data loader\n",
    "@torch.no_grad()\n",
    "def calculate_validation_loss(model, batches_num):\n",
    "  model.eval()\n",
    "  total_loss = 0\n",
    "\n",
    "  validation_iter = iter(validation_dataloader)\n",
    "\n",
    "  for _ in range(batches_num):\n",
    "    input, targets = next(validation_iter)\n",
    "    logits = model(input)\n",
    "\n",
    "    logits_view = logits.view(batch_size * config[\"context_size\"], config[\"vocabulary_size\"])\n",
    "    targets_view = targets.view(batch_size * config[\"context_size\"])\n",
    "      \n",
    "    loss = F.cross_entropy(logits_view, targets_view)\n",
    "\n",
    "    total_loss += loss.item()\n",
    "\n",
    "  average_loss = total_loss / batches_num\n",
    "\n",
    "  return average_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import display, clear_output\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "%matplotlib inline\n",
    "\n",
    "plot_output = widgets.Output()\n",
    "\n",
    "display(plot_output)\n",
    "\n",
    "def update_plot(train_losses, train_steps, validation_losses, validation_steps):\n",
    "\n",
    "  with plot_output:\n",
    "    clear_output(wait=True)  # Clear only the plot output, not the text\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    plt.plot(train_steps, train_losses, label='Training Loss')\n",
    "    plt.plot(validation_steps, validation_losses, label='Validation Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(loc='center left')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Set up lists to store losses for plotting\n",
    "train_losses = []\n",
    "train_steps = []\n",
    "eval_losses = []\n",
    "eval_steps = []\n",
    "\n",
    "\n",
    "for step_num, sample in enumerate(train_dataloader):\n",
    "\n",
    "  model.train()\n",
    "  input, targets = sample\n",
    "  logits = model(input)\n",
    "\n",
    "  logits_view = logits.view(batch_size * config[\"context_size\"], config[\"vocabulary_size\"])\n",
    "  targets_view = targets.view(batch_size * config[\"context_size\"])\n",
    "  \n",
    "  loss = F.cross_entropy(logits_view, targets_view)\n",
    "  # Backward propagation\n",
    "  loss.backward()\n",
    "  # Update model parameters\n",
    "  optimizer.step()\n",
    "  # Set to None to reduce memory usage\n",
    "  optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "  train_losses.append(loss.item())\n",
    "  train_steps.append(step_num)\n",
    "\n",
    "  print(f\"Step {step_num}. Loss {loss.item():.3f}\")\n",
    "\n",
    "  if step_num % evaluation_interval == 0:\n",
    "    print(\"Demo GPT:\\n\" + generate_with_prompt(model, tokenizer, \"\\n\"))\n",
    "\n",
    "    validation_loss = calculate_validation_loss(model, batches_num=10)\n",
    "    eval_losses.append(validation_loss)\n",
    "    eval_steps.append(step_num)\n",
    "\n",
    "    print(f\"Step {step_num}. Validation loss: {validation_loss:.3f}\")\n",
    "\n",
    "\n",
    "  update_plot(train_losses, train_steps, eval_losses, eval_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ai-programming-in-python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
