{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CMWMOaDg-ZBv"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Use CUDA if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "text = Path(\"../../../data/tiny-shakespeare.txt\").read_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "IwwOe-tJ-xcE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text[0:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ap_Ixr0M-0Yv"
   },
   "outputs": [],
   "source": [
    "class CharTokenizer:\n",
    "    def __init__(self, vocabulary):\n",
    "        self.token_id_for_char = {\n",
    "            char: token_id for token_id, char in enumerate(vocabulary)\n",
    "        }\n",
    "        self.char_for_token_id = {\n",
    "            token_id: char for token_id, char in enumerate(vocabulary)\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def train_from_text(text):\n",
    "        vocabulary = set(text)\n",
    "        return CharTokenizer(sorted(list(vocabulary)))\n",
    "\n",
    "    def encode(self, text):\n",
    "        token_ids = []\n",
    "        for char in text:\n",
    "            token_ids.append(self.token_id_for_char[char])\n",
    "        return torch.tensor(token_ids, dtype=torch.long)\n",
    "\n",
    "    def decode(self, token_ids):\n",
    "        chars = []\n",
    "        for token_id in token_ids.tolist():\n",
    "            chars.append(self.char_for_token_id[token_id])\n",
    "        return \"\".join(chars)\n",
    "\n",
    "    def vocabulary_size(self):\n",
    "        return len(self.token_id_for_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T3q9Dj3l-2Ja"
   },
   "outputs": [],
   "source": [
    "tokenizer = CharTokenizer.train_from_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lb1zImZr-4mY"
   },
   "outputs": [],
   "source": [
    "print(tokenizer.encode(\"Hello world\"))\n",
    "print(tokenizer.decode(tokenizer.encode(\"Hello world\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MlTEiIqs-7Uz"
   },
   "outputs": [],
   "source": [
    "print(f\"Vocabulary size: {tokenizer.vocabulary_size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Qal76ig-94U"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class TokenIdsDataset(Dataset):\n",
    "    def __init__(self, data, block_size):\n",
    "        self.data = data\n",
    "        self.block_size = block_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.block_size\n",
    "\n",
    "    def __getitem__(self, pos):\n",
    "        assert pos < len(self.data) - self.block_size\n",
    "\n",
    "        x = self.data[pos : pos + self.block_size]\n",
    "        y = self.data[pos + 1 : pos + 1 + self.block_size]\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"vocabulary_size\": tokenizer.vocabulary_size(),\n",
    "    \"context_size\": 256,\n",
    "    \"d_embed\": 768,\n",
    "    \"heads_num\": 12,\n",
    "    \"layers_num\": 10,\n",
    "    \"dropout_rate\": 0.1,\n",
    "    \"use_bias\": False,\n",
    "}\n",
    "\n",
    "config[\"head_size\"] = config[\"d_embed\"] // config[\"heads_num\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.Q_weights = nn.Linear(\n",
    "            config[\"d_embed\"], config[\"head_size\"], config[\"use_bias\"]\n",
    "        )\n",
    "        self.K_weights = nn.Linear(\n",
    "            config[\"d_embed\"], config[\"head_size\"], config[\"use_bias\"]\n",
    "        )\n",
    "        self.V_weights = nn.Linear(\n",
    "            config[\"d_embed\"], config[\"head_size\"], config[\"use_bias\"]\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(config[\"dropout_rate\"])\n",
    "\n",
    "        casual_attention_mask = torch.tril(\n",
    "            torch.ones(config[\"context_size\"], config[\"context_size\"])\n",
    "        )\n",
    "        self.register_buffer(\"casual_attention_mask\", casual_attention_mask)\n",
    "\n",
    "    def forward(self, input):\n",
    "        batch_size, tokens_num, d_embed = input.shape\n",
    "        Q = self.Q_weights(input)\n",
    "        K = self.K_weights(input)\n",
    "        V = self.V_weights(input)\n",
    "\n",
    "        attention_scores = Q @ K.transpose(1, 2)\n",
    "        attention_scores = attention_scores.masked_fill(\n",
    "            self.casual_attention_mask[:tokens_num, :tokens_num] == 0, -torch.inf\n",
    "        )\n",
    "        attention_scores = attention_scores / (K.shape[-1] ** 0.5)\n",
    "        attention_scores = torch.softmax(attention_scores, dim=-1)\n",
    "        attention_scores = self.dropout(attention_scores)\n",
    "\n",
    "        return attention_scores @ V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.rand(8, config[\"context_size\"], config[\"d_embed\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ah = AttentionHead(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = ah(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        heads_list = [AttentionHead(config) for _ in range(config[\"heads_num\"])]\n",
    "        self.heads = nn.ModuleList(heads_list)\n",
    "\n",
    "        self.linear = nn.Linear(config[\"d_embed\"], config[\"d_embed\"])\n",
    "        self.dropout = nn.Dropout(config[\"dropout_rate\"])\n",
    "\n",
    "    def forward(self, input):\n",
    "        heads_outputs = [head(input) for head in self.heads]\n",
    "\n",
    "        scores_change = torch.cat(heads_outputs, dim=-1)\n",
    "\n",
    "        scores_change = self.linear(scores_change)\n",
    "        return self.dropout(scores_change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mha = MultiHeadAttention(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.rand(8, config[\"context_size\"], config[\"d_embed\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = mha(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear_layers = nn.Sequential(\n",
    "            nn.Linear(config[\"d_embed\"], config[\"d_embed\"] * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(config[\"d_embed\"] * 4, config[\"d_embed\"]),\n",
    "            nn.Dropout(config[\"dropout_rate\"]),\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.linear_layers(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff = FeedForward(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.rand(8, config[\"context_size\"], config[\"d_embed\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ouptut = ff(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.multi_head = MultiHeadAttention(config)\n",
    "        self.layer_norm_1 = nn.LayerNorm(config[\"d_embed\"])\n",
    "\n",
    "        self.feed_forward = FeedForward(config)\n",
    "        self.layer_norm_2 = nn.LayerNorm(config[\"d_embed\"])\n",
    "\n",
    "    def forward(self, input):\n",
    "        residual = input\n",
    "        x = self.multi_head(self.layer_norm_1(input))\n",
    "        x = x + residual\n",
    "\n",
    "        residual = x\n",
    "        x = self.feed_forward(self.layer_norm_2(x))\n",
    "        return x + residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = Block(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ouptut = b(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DemoGPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_embedding_layer = nn.Embedding(\n",
    "            config[\"vocabulary_size\"], config[\"d_embed\"]\n",
    "        )\n",
    "        self.positional_embedding_layer = nn.Embedding(\n",
    "            config[\"context_size\"], config[\"d_embed\"]\n",
    "        )\n",
    "\n",
    "        blocks = [Block(config) for _ in range(config[\"layers_num\"])]\n",
    "        self.layers = nn.Sequential(*blocks)\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(config[\"d_embed\"])\n",
    "        self.unembedding = nn.Linear(\n",
    "            config[\"d_embed\"], config[\"vocabulary_size\"], bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, token_ids):\n",
    "        batch_size, tokens_num = token_ids.shape\n",
    "\n",
    "        x = self.token_embedding_layer(token_ids)\n",
    "        sequence = torch.arange(tokens_num, device=device)\n",
    "        x = x + self.positional_embedding_layer(sequence)\n",
    "\n",
    "        x = self.layers(x)\n",
    "        x = self.layer_norm(x)\n",
    "        x = self.unembedding(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DemoGPT(config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(tokenizer.encode(\"Hi\").unsqueeze(dim=0).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, prompt_ids, max_tokens):\n",
    "    output_ids = prompt_ids\n",
    "    for _ in range(max_tokens):\n",
    "        if output_ids.shape[1] >= config[\"context_size\"]:\n",
    "            break\n",
    "        with torch.no_grad():\n",
    "            logits = model(output_ids)\n",
    "\n",
    "        logits = logits[:, -1, :]\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        # Sample a random token given the softmax distribution\n",
    "        next_token_id = torch.multinomial(probs, num_samples=1)\n",
    "        # Add new token to the output, and repeat the process\n",
    "        output_ids = torch.cat([output_ids, next_token_id], dim=-1)\n",
    "    return output_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_prompt(model, tokenizer, prompt, max_tokens=100):\n",
    "    model.eval()\n",
    "\n",
    "    prompt = tokenizer.encode(prompt).unsqueeze(dim=0).to(device)\n",
    "\n",
    "    return tokenizer.decode(generate(model, prompt, max_tokens=max_tokens)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_with_prompt(model, tokenizer, \"First Citizen:\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_iterations = 500\n",
    "evaluation_interval = 10\n",
    "learning_rate = 4e-4\n",
    "train_split = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 - Split Data into Training and Validation Dataset\n",
    "\n",
    "tokenized_text = tokenizer.encode(text).to(device)\n",
    "# TODO: Get number of tokens in the training dataset. Should be train_split * number_of_tokens\n",
    "# TODO: Split data into training and validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 - Create Validation Dataset\n",
    "\n",
    "train_dataset = TokenIdsDataset(train_data, config[\"context_size\"])\n",
    "# TODO: Create a validation dataset from the validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 - Create Validation DataLoader\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler\n",
    "\n",
    "train_sampler = RandomSampler(\n",
    "    train_dataset, num_samples=batch_size * train_iterations, replacement=True\n",
    ")\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size, sampler=train_sampler\n",
    ")\n",
    "\n",
    "validation_sampler = RandomSampler(validation_dataset, replacement=True)\n",
    "# TODO: Create validation data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4 - Calculate Validation Loss\n",
    "\n",
    "\n",
    "# Compute validation loss for the model using \"batches_num\" batches\n",
    "# from the validation data loader\n",
    "@torch.no_grad()\n",
    "def calculate_validation_loss(model, batches_num):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "\n",
    "    # TODO: Create an iterator for the validation data loader\n",
    "\n",
    "    for _ in range(batches_num):\n",
    "        idx, targets = next(validation_iter)\n",
    "        logits = model(idx)\n",
    "\n",
    "        # TODO: Call \"next\" function to get input and targets from the iterator\n",
    "        # TODO: Using the model compute logits given the input\n",
    "\n",
    "        # TODO: Use the \"view\" method to convert logits and targets so we could use the \"cross_entropy\" function\n",
    "        # It should be similar to how we do it in the training code\n",
    "\n",
    "        # TODO: calculate cross entropy using logits and target data\n",
    "\n",
    "        # TODO: Add loss to the \"total_loss\" variable\n",
    "        # Note: you would need to use the \"item()\" method to convert a tensor to a number\n",
    "\n",
    "    average_loss = total_loss / batches_num\n",
    "\n",
    "    return average_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5 - Update the Training Loop\n",
    "\n",
    "import os\n",
    "from IPython.display import display, clear_output\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "%matplotlib inline\n",
    "\n",
    "plot_output = widgets.Output()\n",
    "\n",
    "display(plot_output)\n",
    "\n",
    "def update_plot(train_losses, train_steps, validation_losses, validation_steps):\n",
    "\n",
    "  with plot_output:\n",
    "    clear_output(wait=True)  # Clear only the plot output, not the text\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    plt.plot(train_steps, train_losses, label='Training Loss')\n",
    "    plt.plot(validation_steps, validation_losses, label='Validation Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(loc='center left')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Set up lists to store losses for plotting\n",
    "train_losses = []\n",
    "train_steps = []\n",
    "eval_losses = []\n",
    "eval_steps = []\n",
    "\n",
    "\n",
    "for step_num, sample in enumerate(train_dataloader):\n",
    "\n",
    "  model.train()\n",
    "  input, targets = sample\n",
    "  logits = model(input)\n",
    "\n",
    "  logits_view = logits.view(batch_size * config[\"context_size\"], config[\"vocabulary_size\"])\n",
    "  targets_view = targets.view(batch_size * config[\"context_size\"])\n",
    "  \n",
    "  loss = F.cross_entropy(logits_view, targets_view)\n",
    "  # Backward propagation\n",
    "  loss.backward()\n",
    "  # Update model parameters\n",
    "  optimizer.step()\n",
    "  # Set to None to reduce memory usage\n",
    "  optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "  train_losses.append(loss.item())\n",
    "  train_steps.append(step_num)\n",
    "  # TODO: Append training loss\n",
    "  # TODO: Append training step\n",
    "\n",
    "  print(f\"Step {step_num}. Loss {loss.item():.3f}\")\n",
    "\n",
    "  if step_num % evaluation_interval == 0:\n",
    "    print(\"Demo GPT:\\n\" + generate_with_prompt(model, tokenizer, \"\\n\"))\n",
    "\n",
    "    validation_loss = calculate_validation_loss(model, batches_num=10)\n",
    "    # TODO: Append validation loss\n",
    "    # TODO: Append validation step\n",
    "\n",
    "    print(f\"Step {step_num}. Validation loss: {validation_loss:.3f}\")\n",
    "\n",
    "\n",
    "  update_plot(train_losses, train_steps, eval_losses, eval_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ai-programming-in-python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
