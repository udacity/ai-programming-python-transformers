{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ulgnkbbvEt6u"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "cuda_is_available = torch.cuda.is_available()\n",
        "\n",
        "if cuda_is_available:\n",
        "    print(\"All good!\")\n",
        "else:\n",
        "    print(\"CUDA is NOT available!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7R6RnqQu0-P9"
      },
      "outputs": [],
      "source": [
        "prompt = \"During the latest presentation OpenAI\"\n",
        "model = \"openai-community/gpt2-large\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hkVKzDegAItu"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load the text generation pipeline\n",
        "text_generator = pipeline(\"text-generation\", model=model, device=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_xUAq3XiAN1z"
      },
      "outputs": [],
      "source": [
        "# Generate text\n",
        "generated_texts = text_generator(prompt, max_length=100, num_return_sequences=1)\n",
        "\n",
        "print(f\"Generated text:\\n{generated_texts[0]['generated_text']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rGcDDPcz1Ln5"
      },
      "outputs": [],
      "source": [
        "for do_sample in [False, True]:  # Greedy Search  # Multinomial sampling\n",
        "    generated_texts = text_generator(\n",
        "        prompt, max_length=100, num_return_sequences=1, do_sample=do_sample, num_beams=1\n",
        "    )\n",
        "\n",
        "    print(\"-----------------------------------\")\n",
        "    print(\"Parameters:\")\n",
        "    print(\"-----------------------------------\")\n",
        "    print(f\"do_sample={do_sample}\")\n",
        "    print(\"-----------------------------------\")\n",
        "    print(\"Generation:\")\n",
        "    print(\"-----------------------------------\")\n",
        "    print(generated_texts[0][\"generated_text\"])\n",
        "    print(\"-----------------------------------\")\n",
        "    print(\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uo3CC3yk4QmC"
      },
      "outputs": [],
      "source": [
        "# Beam-search strategy\n",
        "\n",
        "for beams in [1, 2, 4, 8]:\n",
        "    generated_texts = text_generator(\n",
        "        prompt, max_length=100, num_return_sequences=1, do_sample=False, num_beams=beams\n",
        "    )\n",
        "\n",
        "    print(\"-----------------------------------\")\n",
        "    print(\"Parameters:\")\n",
        "    print(\"-----------------------------------\")\n",
        "    print(f\"num_beams={beams}\")\n",
        "    print(\"-----------------------------------\")\n",
        "    print(\"Generation:\")\n",
        "    print(\"-----------------------------------\")\n",
        "    print(generated_texts[0][\"generated_text\"])\n",
        "    print(\"-----------------------------------\")\n",
        "    print(\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K96hVRPC6MUR"
      },
      "outputs": [],
      "source": [
        "# Beam-search multinomial sampling\n",
        "\n",
        "for beams in [1, 2, 4, 8]:\n",
        "    generated_texts = text_generator(\n",
        "        prompt, max_length=100, num_return_sequences=1, do_sample=True, num_beams=beams\n",
        "    )\n",
        "\n",
        "    print(\"-----------------------------------\")\n",
        "    print(\"Parameters:\")\n",
        "    print(\"-----------------------------------\")\n",
        "    print(f\"num_beams={beams}\")\n",
        "    print(\"-----------------------------------\")\n",
        "    print(\"Generation:\")\n",
        "    print(\"-----------------------------------\")\n",
        "    print(generated_texts[0][\"generated_text\"])\n",
        "    print(\"-----------------------------------\")\n",
        "    print(\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f5R7GeD3AUgp"
      },
      "outputs": [],
      "source": [
        "# Change the top-k parameter\n",
        "\n",
        "for k in [1, 5, 10, 50, 100, 500]:\n",
        "    generated_texts = text_generator(\n",
        "        prompt, max_length=100, num_return_sequences=1, top_k=k\n",
        "    )\n",
        "\n",
        "    print(\"-----------------------------------\")\n",
        "    print(\"Parameters:\")\n",
        "    print(\"-----------------------------------\")\n",
        "    print(f\"top_k={k}\")\n",
        "    print(\"-----------------------------------\")\n",
        "    print(\"Generation:\")\n",
        "    print(\"-----------------------------------\")\n",
        "    print(generated_texts[0][\"generated_text\"])\n",
        "    print(\"-----------------------------------\")\n",
        "    print(\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yXOuaxe0RJwf"
      },
      "outputs": [],
      "source": [
        "# Change temperature\n",
        "\n",
        "for temp in [0.1, 1.0, 2.0, 3.0]:\n",
        "    generated_texts = text_generator(\n",
        "        prompt, max_length=100, num_return_sequences=1, temperature=temp\n",
        "    )\n",
        "\n",
        "    print(\"-----------------------------------\")\n",
        "    print(\"Parameters:\")\n",
        "    print(\"-----------------------------------\")\n",
        "    print(f\"temperature={temp}\")\n",
        "    print(\"-----------------------------------\")\n",
        "    print(\"Generation:\")\n",
        "    print(\"-----------------------------------\")\n",
        "    print(generated_texts[0][\"generated_text\"])\n",
        "    print(\"-----------------------------------\")\n",
        "    print(\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UUiu5ZdT9ArW"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
