{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ulgnkbbvEt6u"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "cuda_is_available = torch.cuda.is_available()\n",
        "\n",
        "if cuda_is_available:\n",
        "  print(\"All good!\")\n",
        "else:\n",
        "  print(\"CUDA is NOT available!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7R6RnqQu0-P9"
      },
      "outputs": [],
      "source": [
        "# You can use this prompt or try something else!\n",
        "prompt = \"During the latest presentation OpenAI\"\n",
        "# A good model for this exercise, but feel free to use another model: https://huggingface.co/models?pipeline_tag=text-generation&sort=downloads\n",
        "model = \"openai-community/gpt2-large\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hkVKzDegAItu"
      },
      "outputs": [],
      "source": [
        "# Step 1 - Set Up the Text Generation Pipeline\n",
        "\n",
        "# TODO: Import \"pipeline\" function\n",
        "\n",
        "# TODO: Create a pipeline for text generation, selected model, and \"device=0\" (to use CUDA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_xUAq3XiAN1z"
      },
      "outputs": [],
      "source": [
        "# Step 2 - Generate text with the default settings\n",
        "\n",
        "# TODO: Generate text with the selected prompt\n",
        "# Recommended parameters:\n",
        "# max_length=100 - generate up to 100 tokens\n",
        "# num_return_sequences=1 - only return one generated sequence\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rGcDDPcz1Ln5"
      },
      "outputs": [],
      "source": [
        "# Step 3 - Experiment with Different Parameters\n",
        "\n",
        "# Now try different parameters\n",
        "\n",
        "# TODO: Try text generation with \"do_sample\" parameter equal to `True` or `False`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uo3CC3yk4QmC"
      },
      "outputs": [],
      "source": [
        "# TODO: Try text generation using \"Beam-search strategy\"\n",
        "# Parameters:\n",
        "# do_sample=False\n",
        "# num_beams - try different values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K96hVRPC6MUR"
      },
      "outputs": [],
      "source": [
        "# TODO: Try text generation using \"Beam-search multinomial sampling\"\n",
        "# Parameters:\n",
        "# do_sample=True\n",
        "# num_beams - try different values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f5R7GeD3AUgp"
      },
      "outputs": [],
      "source": [
        "# TODO: Try text generation with different \"top_k\" values. E.g. from 1 to 500"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yXOuaxe0RJwf"
      },
      "outputs": [],
      "source": [
        "# TODO: Try text generation with different \"temperature\" values. E.g. from 0.1 to 3.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UUiu5ZdT9ArW"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
